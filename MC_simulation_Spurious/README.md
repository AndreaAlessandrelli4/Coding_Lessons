# Hopfield & TAM Network Simulations

This repository provides a Python implementation (using TensorFlow) of Monte Carlo simulations for **Hopfield neural networks** and **TAM networks**. The code allows for the generation of synaptic matrices using the Hebbian learning rule and simulates neuron dynamics under different update schemes.

## Requirements

- Python â‰¥ 3.7
- `numpy`
- `tensorflow`
- `tqdm`

To install the required packages:

```bash
pip install numpy tensorflow tqdm
```

---

## Code Overview

### ðŸ“Œ Pattern Generation

```python
gen_patterns(steps, N, K)
```

Generates a tensor of Rademacher-distributed patterns (values in {-1, +1}).

- `steps`: number of parallel Monte Carlo simulations
- `N`: length of each pattern
- `K`: number of patterns
- **Returns**: TensorFlow tensor of shape `(steps, K, N)`

---

### ðŸ“Œ Hebbian Synaptic Matrix

```python
Hebb_J(Î¾)
```

Computes the Hebbian synaptic matrix for each simulation.

- `Î¾`: pattern tensor generated by `gen_patterns`
- **Returns**: TensorFlow tensor of shape `(steps, N, N)`

---

## ðŸ§  Hopfield_Network

Class to simulate the standard Hopfield neural network.

### Main Methods

#### `prepare(steps_input, N_input, K_input)`
Initializes the parameters and generates both the patterns and the synaptic matrix.

#### `dynamics(mu, r, Î², loop, updates, mode="parallel", verbose=True)`
Performs the Monte Carlo simulation.

- `mu`: index of the pattern to retrieve
- `r`: initial noise level
- `Î²`: inverse temperature
- `loop`: number of initial configurations per simulation
- `updates`: number of Monte Carlo update steps
- `mode`: `"parallel"` (default) or `"serial"` neuron updates
- `verbose`: enables progress bar if `True`

---

## ðŸ§  TAM_Network

Class to simulate the TAM (Tensor Associative Memory) network, which generalizes Hopfield dynamics using higher-order associative tensors.

### Main Methods

#### `prepare(steps_input, N_input, K_input, L_input)`
Initializes the TAM network parameters and generates the pattern tensor.

#### `compute_fields(input_field, Ïƒ, P)`
Computes the internal fields for the TAM update rule.

- `input_field`: external input field
- `Ïƒ`: current spin configurations
- `P`: order of the interaction tensor

#### `dynamics(P, Î², Î», h, updates, verbose=True)`
Runs the TAM network dynamics.

- `P`: order of the associative interaction
- `Î²`: inverse temperature
- `Î»`: regularization coefficient for higher-order terms
- `h`: external input field strength
- `updates`: number of update iterations
- `verbose`: shows progress bar if `True`

---

## ðŸ“‚ Project Structure

```
.
â”œâ”€â”€ main.py              # Contains the Hopfield and TAM implementations
â”œâ”€â”€ README.md            # Project documentation
```

---

## ðŸ”¬ Notes

- This implementation is designed for educational and research purposes.
- The Hopfield and TAM models here are simulated in parallel for multiple Monte Carlo steps.
- For further theoretical background, see literature on associative memory networks and high-order Hopfield models.

  ---

## ðŸ“š References

- [Albanese L. et al. (2022) "Replica symmetry breaking in dense hebbian neural networks."](https://doi.org/10.1007/s10955-022-02966-8)
- [Agliari E. et al. (2025) "Multi-channel pattern reconstruction through $ L $-directional associative memories."](https://doi.org/10.48550/arXiv.2503.06274)
- [Agliari E. et al. (2025) "Generalized hetero-associative neural networks."](https://doi.org/10.1088/1742-5468/ada918)
